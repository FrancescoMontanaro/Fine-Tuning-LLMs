{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f123fe2",
   "metadata": {},
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.utils import get_device, set_seed\n",
    "from src.hf import hf_login, load_hf_dataset, dataset_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae9fc2",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "hf_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b82ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aded5",
   "metadata": {},
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d379eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M\" # The model ID\n",
    "dataset_name = \"banghua/DL-SFT-Dataset\" # The dataset name on Hugging Face Hub\n",
    "model_path = Path().resolve().parent.parent / \"saved_models\" / f\"{model_id.split('/')[-1]}_instruct\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e025e",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face Hub\n",
    "dataset = load_hf_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bad1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame for easier manipulation\n",
    "dataset_df = dataset_to_pandas(dataset)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(dataset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf54b2",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd83aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_dataset, test_dataset = dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130d439",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Define the chat template if not already defined\n",
    "if not tokenizer.chat_template:\n",
    "\ttokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "\t{% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "\t{% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "\t{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "\t{% endif %}\n",
    "\t{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b550399",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b28e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111aa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e00c5",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# SFTTrainer config \n",
    "sft_config = SFTConfig(\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    logging_steps = 10,\n",
    "    eval_strategy = \"steps\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "\tbf16 = bf16,\n",
    "\tweight_decay = 0.01,\n",
    " \tlr_scheduler_type = \"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args = sft_config,\n",
    "    train_dataset = train_dataset, \n",
    "    eval_dataset = test_dataset,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf52717",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message: str, system_message: Optional[str] = None, max_new_tokens: int = 100) -> str:\n",
    "    # Format chat using tokenizer's chat template\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "    # Generate the prompt using the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True,\n",
    "        enable_thinking = False\n",
    "    )\n",
    "\n",
    "    # Tokenize the prompt and move to the appropriate device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Disable gradient calculations for inference\n",
    "    with torch.no_grad():\n",
    "        # Generate the model outputs\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            do_sample = False,\n",
    "            pad_token_id = tokenizer.eos_token_id,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "    # Decode the generated tokens to get the response\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Return the generated response\n",
    "    return response\n",
    "\n",
    "\n",
    "def test_model_with_questions(model, tokenizer, questions: list[str], system_message: Optional[str] = None, title: str = \"Model Output\"):\n",
    "    # Test the model with a list of questions and print the responses\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    \n",
    "    # Iterate through each question and generate a response\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        # Generate the response\n",
    "        response = generate_responses(model, tokenizer, question, system_message)\n",
    "        \n",
    "        # Print the input question and the model's response\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897084fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of questions to test the model\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]\n",
    "\n",
    "# Test the fine-tuned model with the defined questions\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, title=\"Base Model (After SFT) Output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
