{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbMjkhsSyq-"
   },
   "source": [
    "### Installing and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QV9rVdkGEFFI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Any\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, TextStreamer\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.utils import get_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pz1dCNVaXdjy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Extract the hugging face token from the user data\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Check if the HF token has been provided\n",
    "if not HF_TOKEN:\n",
    "  # Raise an exception if the HF token was not provided\n",
    "  raise Exception(\"Token is not set. Please save the token first.\")\n",
    "\n",
    "# Authenticate with hugging face\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Login successful\n",
    "print(\"Successfully logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHtrAS4G_3c"
   },
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_OMAcVUtEWyv"
   },
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "max_length = 448 # Maximum length of the sequences\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\" # The model ID of the Llama model\n",
    "dataset_path = Path().resolve().parent.parent / \"datasets\" / \"arxiv_dataset.csv\" # Path to the dataset\n",
    "adapter_path = Path().resolve().parent.parent / \"saved_models\" / \"papers_category_classifier_adapter\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E5A58TsXOdcF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJtYT9AAJA9Z"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UGd3OTw_JChU"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(\n",
    "    dataset_path,\n",
    "    delimiter = \"|\",\n",
    "    quoting = 3,  # Handle quotes around text\n",
    "    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dataset = dataset[[\n",
    "    \"summary\", # Feature\n",
    "    \"category_description\" # Label\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "obTlSyEdJSyN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>category_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present PERSE, a method for building an ani...</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We propose action-agnostic point-level (AAPL) ...</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We study $\\textit{sparse singular value certif...</td>\n",
       "      <td>Data Structures and Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mixture-of-Agents (MoA) has recently been prop...</td>\n",
       "      <td>Information Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce self-invoking code generation, a ...</td>\n",
       "      <td>Software Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  We present PERSE, a method for building an ani...   \n",
       "1  We propose action-agnostic point-level (AAPL) ...   \n",
       "2  We study $\\textit{sparse singular value certif...   \n",
       "3  Mixture-of-Agents (MoA) has recently been prop...   \n",
       "4  We introduce self-invoking code generation, a ...   \n",
       "\n",
       "                      category_description  \n",
       "0  Computer Vision and Pattern Recognition  \n",
       "1  Computer Vision and Pattern Recognition  \n",
       "2           Data Structures and Algorithms  \n",
       "3                       Information Theory  \n",
       "4                     Software Engineering  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a subset of the samples\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YwNOg1CHqJp"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FDgHdDxaEa5k"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWe7YFh9KM47"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FYydkXj3KxYe"
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = hf_dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zTYrCiEuxKo"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_chat(user_text: str, answer_text: str) -> tuple[list[int], list[int], list[int]]:\n",
    "    # Build full conversation with target\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Category: {answer_text}\"}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    full_text = tokenizer.apply_chat_template(conversation, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "    # Build prompt only (without the answer) and apply the chat template\n",
    "    prompt_only = [{\"role\": \"user\", \"content\": user_text}]\n",
    "    prompt_text = tokenizer.apply_chat_template(prompt_only, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "\t# Tokenize both full and prompt texts\n",
    "    full = tokenizer(full_text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    prompt = tokenizer(prompt_text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\t# Extract input ids and attention masks\n",
    "    input_ids = full[\"input_ids\"]\n",
    "    attn = full[\"attention_mask\"]\n",
    "\n",
    "\t# Create labels, initialized to -100 (ignore index)\n",
    "    labels = [-100] * len(input_ids)\n",
    "    \n",
    "    # Determine the starting index of the assistant's response\n",
    "    start = len(tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"])\n",
    "    \n",
    "    # Fill labels with input ids for the assistant portion only, ignore padding\n",
    "    for i in range(start, len(input_ids)):\n",
    "        if attn[i] == 1:\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "\t# Return the input ids, attention mask, and labels\n",
    "    return input_ids, attn, labels\n",
    "\n",
    "def preprocess(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Preprocess the examples to build input ids, attention masks, and labels\n",
    "    inputs, masks, labels = [], [], []\n",
    "    \n",
    "    # Iterate through each example and build the chat inputs\n",
    "    for u, y in zip(examples[\"summary\"], examples[\"category_description\"]):\n",
    "        # Build chat inputs\n",
    "        ids, attn, labs = build_chat(u, y)\n",
    "        \n",
    "        # Append to the respective lists\n",
    "        inputs.append(ids)\n",
    "        masks.append(attn)\n",
    "        labels.append(labs)\n",
    "        \n",
    "\t# Return the processed inputs as a dictionary\n",
    "    return {\"input_ids\": inputs, \"attention_mask\": masks, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RevlAGKTwuWC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7c1905ac0c4662bf1417433285618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c69c4d3a1a4453a9dbaea367b3f9707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"summary\",\"category_description\"])\n",
    "tokenized_test_dataset  = test_dataset.map(preprocess,  batched=True, remove_columns=[\"summary\",\"category_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hwZ4CvuLyz4p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence, capable of processing and understanding extensive human knowledge to enhance problem-solving across various domains. This paper explores the potential of LLMs to drive the discovery of symbolic solutions within scientific and engineering disciplines, where such solutions are crucial for advancing theoretical and practical applications. We propose a novel framework that utilizes LLMs in an evolutionary search methodology, augmented by a dynamic knowledge library that integrates and refines insights in an \\textit{open-ended manner}. This approach aims to tackle the dual challenges of efficiently navigating complex symbolic representation spaces and leveraging both existing and newly generated knowledge to foster open-ended innovation. By enabling LLMs to interact with and expand upon a knowledge library, we facilitate the continuous generation of novel solutions in diverse forms such as language, code, and mathematical expressions. Our experimental results demonstrate that this method not only enhances the efficiency of searching for symbolic solutions but also supports the ongoing discovery process, akin to human scientific endeavors. This study represents a first effort in conceptualizing the search for symbolic solutions as a lifelong, iterative process, marking a significant step towards harnessing AI in the perpetual pursuit of scientific and engineering breakthroughs. We have open-sourced our code and data, please visit \\url{https://github.com/pgg3/CoEvo} for more information.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Category: Artificial Intelligence<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Select a random training sample\n",
    "random_sample = random.choice(tokenized_train_dataset)\n",
    "\n",
    "# Print a random sequence\n",
    "print(tokenizer.decode(random_sample[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jVoBtk0HwS9"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rCk7e6F5w3Id"
   },
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BGRRZ6OvE8Ro"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77be5f1b48164febb41951f0387bcd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4ua74D42HIFj"
   },
   "outputs": [],
   "source": [
    "# LoRA (Low-rank adaptation configurations)\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,                        # Rank of the LoRA matrices\n",
    "    lora_alpha = 32,               # Alpha parameter for scaling\n",
    "    use_rslora = True,             # Use RSLora\n",
    "    lora_dropout = 0.1,            # Dropout probability\n",
    "    target_modules = [             # Target modules to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JPgCfrhdH34l"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA (Low-rank adaptation) to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LWh5J_2lPX_z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n"
     ]
    }
   ],
   "source": [
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bxcI8ceiTmzx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddnJy6u3IILa"
   },
   "source": [
    "### Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "n6IqtdVEIJ46"
   },
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./checkpoints/papers_category_classifier\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 3e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 10,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 50,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to = \"none\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "    bf16 = bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX8Id63uNJ0D"
   },
   "outputs": [],
   "source": [
    "# Instantiate the trainer to train the model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_test_dataset\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Pretty print the training results\n",
    "print(trainer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKZO4iY6j5V"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IG859UBQ6n_L"
   },
   "outputs": [],
   "source": [
    "# Saving the adapter to the destination path\n",
    "model.save_pretrained(str(adapter_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKGMC3q08t6o"
   },
   "source": [
    "### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j0_DtdlICmI9"
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CGEk88YD838p"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ac4d1ceb9f4807b9e49a0fbb836333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model first\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter and attach it to the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UsqhsFpN9qA2"
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI9Bm9SoDEOS"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xnv0OIRZEpdO"
   },
   "outputs": [],
   "source": [
    "# Tokenize a sample input for chat-like generation\n",
    "summary = \"The transportation industry is experiencing vast digitalization as a plethora of technologies are being implemented to improve efficiency, functionality, and safety. Although technological advancements bring many benefits to transportation, integrating cyberspace across transportation sectors has introduced new and deliberate cyber threats. In the past, public agencies assumed digital infrastructure was secured since its vulnerabilities were unknown to adversaries. However, with the expansion of cyberspace, this assumption has become invalid. With the rapid advancement of wireless technologies, transportation systems are increasingly interconnected with both transportation and non-transportation networks in an internet-of-things ecosystem, expanding cyberspace in transportation and increasing threats and vulnerabilities. This study investigates some prominent reasons for the increase in cyber vulnerabilities in transportation. In addition, this study presents various collaborative strategies among stakeholders that could help improve cybersecurity in the transportation industry. These strategies address programmatic and policy aspects and suggest avenues for technological research and development. The latter highlights opportunities for future research to enhance the cybersecurity of transportation systems and infrastructure by leveraging hybrid approaches and emerging technologies.\"\n",
    "\n",
    "# Compose the chat-like prompt\n",
    "prompt = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": summary},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Apply chat template if supported\n",
    "messages = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False\n",
    ")\n",
    "\n",
    "# Tokenize the formatted prompt\n",
    "inputs = tokenizer(\n",
    "    messages,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cdWiC7QUDDK1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Systems and Control<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Generate the responses\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 16,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PJMObYBnDJwX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systems and Control\n"
     ]
    }
   ],
   "source": [
    "# Decode the model output\n",
    "gen_ids = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "# Extract the generated category from the response\n",
    "category = generated_text.split(\"Category:\", 1)[-1].strip() if \"Category:\" in generated_text else generated_text.strip()\n",
    "\n",
    "# Print the response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHq4qylDQ36x8M54uxshR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
