{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbMjkhsSyq-"
   },
   "source": [
    "### Installing and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV9rVdkGEFFI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Any\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.utils import get_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "dotenv.load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz1dCNVaXdjy"
   },
   "outputs": [],
   "source": [
    "# Extract the hugging face token from the user data\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Check if the HF token has been provided\n",
    "if not HF_TOKEN:\n",
    "  # Raise an exception if the HF token was not provided\n",
    "  raise Exception(\"Token is not set. Please save the token first.\")\n",
    "\n",
    "# Authenticate with hugging face\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Login successful\n",
    "print(\"Successfully logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHtrAS4G_3c"
   },
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OMAcVUtEWyv"
   },
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "max_length = 128 # Maximum length of the sequences\n",
    "model_id = \"Qwen/Qwen3-0.6B\" # The model ID of the Llama model\n",
    "dataset_path = Path().resolve().parent.parent / \"datasets\" / \"arxiv_dataset.csv\" # Path to the dataset\n",
    "adapter_path = Path().resolve().parent.parent / \"saved_models\" / \"papers_category_classifier_adapter\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5A58TsXOdcF"
   },
   "outputs": [],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJtYT9AAJA9Z"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGd3OTw_JChU"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(\n",
    "    dataset_path,\n",
    "    delimiter = \"|\",\n",
    "    quoting = 3,  # Handle quotes around text\n",
    "    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dataset = dataset[[\n",
    "    \"summary\", # Feature\n",
    "    \"category_description\" # Label\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obTlSyEdJSyN"
   },
   "outputs": [],
   "source": [
    "# Show a subset of the samples\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YwNOg1CHqJp"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDgHdDxaEa5k"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token to the end of the sequence\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWe7YFh9KM47"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYydkXj3KxYe"
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = hf_dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zTYrCiEuxKo"
   },
   "outputs": [],
   "source": [
    "def preprocess(examples: Dict[str, Any], max_length: int = 128) -> Dict[str, Any]:\n",
    "    # Define the expected response template\n",
    "    response_template = lambda category: f\"Category: {category}{tokenizer.eos_token}\"\n",
    "    \n",
    "    # Create the prompts\n",
    "    prompts = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": summary},\n",
    "            {\"role\": \"assistant\", \"content\": response_template(category)}\n",
    "        ]\n",
    "        for summary, category in zip(examples[\"summary\"], examples[\"category_description\"])\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    texts = tokenizer.apply_chat_template(\n",
    "        prompts, add_generation_prompt=False, tokenize=False\n",
    "    )\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = max_length\n",
    "    )\n",
    "\n",
    "    # Tokenize the targets\n",
    "    targets = [response_template(category) for category in examples[\"category_description\"]]\n",
    "    tgt_enc = tokenizer(\n",
    "        targets,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = max_length\n",
    "    )\n",
    "\n",
    "    # Extract input IDs and create labels\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels = []\n",
    "    for ids, tgt_ids in zip(input_ids, tgt_enc[\"input_ids\"]):\n",
    "        # Create a label array initialized to -100\n",
    "        lbl = [-100] * len(ids)\n",
    "        \n",
    "        # Determine the padding token ID\n",
    "        try:\n",
    "            pad_id = tokenizer.pad_token_id\n",
    "        except:\n",
    "            pad_id = tokenizer.eos_token_id\n",
    "            \n",
    "        # Compute the length of the target without padding\n",
    "        tgt_len = len(tgt_ids) - tgt_ids[::-1].index(pad_id) if pad_id in tgt_ids else len(tgt_ids)\n",
    "\n",
    "        # Find the starting index to copy the target IDs\n",
    "        start = len(ids) - tgt_len\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "            \n",
    "        # Copy the target IDs into the label array\n",
    "        lbl[start:] = tgt_ids[-(len(ids)-start):]\n",
    "        \n",
    "        # Mask the padding tokens in the labels\n",
    "        labels.append([(-100 if t == pad_id else t) for t in lbl])\n",
    "\n",
    "    # Return the processed inputs\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RevlAGKTwuWC"
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "tokenized_train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"summary\", \"category_description\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"summary\", \"category_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwZ4CvuLyz4p"
   },
   "outputs": [],
   "source": [
    "# Select a random training sample\n",
    "random_sample = random.choice(tokenized_train_dataset)\n",
    "\n",
    "# Print a random sequence\n",
    "print(\"INPUT SEQUENCE\")\n",
    "print(\"-\"*15)\n",
    "print(tokenizer.decode(random_sample[\"input_ids\"]))\n",
    "\n",
    "# Print a random sequence\n",
    "print(\"\\nOUTPUT SEQUENCE\")\n",
    "print(\"-\"*15)\n",
    "print(tokenizer.decode([\n",
    "    tokenizer.pad_token_id if token == -100 else token\n",
    "    for token in random_sample[\"labels\"]\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jVoBtk0HwS9"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCk7e6F5w3Id"
   },
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGRRZ6OvE8Ro"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ua74D42HIFj"
   },
   "outputs": [],
   "source": [
    "# LoRA (Low-rank adaptation configurations)\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,                        # Rank of the LoRA matrices\n",
    "    lora_alpha = 32,               # Alpha parameter for scaling\n",
    "    use_rslora = True,             # Use RSLora\n",
    "    lora_dropout = 0.1,            # Dropout probability\n",
    "    target_modules = [             # Target modules to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPgCfrhdH34l"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA (Low-rank adaptation) to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWh5J_2lPX_z"
   },
   "outputs": [],
   "source": [
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxcI8ceiTmzx"
   },
   "outputs": [],
   "source": [
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddnJy6u3IILa"
   },
   "source": [
    "### Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6IqtdVEIJ46"
   },
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./checkpoints/papers_category_classifier\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 3e-5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    num_train_epochs = 10,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 50,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to = \"none\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "    bf16 = bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX8Id63uNJ0D"
   },
   "outputs": [],
   "source": [
    "# Instantiate the trainer to train the model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_test_dataset\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Pretty print the training results\n",
    "print(trainer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKZO4iY6j5V"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG859UBQ6n_L"
   },
   "outputs": [],
   "source": [
    "# Saving the adapter to the destination path\n",
    "model.save_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKGMC3q08t6o"
   },
   "source": [
    "### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_DtdlICmI9"
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGEk88YD838p"
   },
   "outputs": [],
   "source": [
    "# Load the base model first\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter and attach it to the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsqhsFpN9qA2"
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI9Bm9SoDEOS"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnv0OIRZEpdO"
   },
   "outputs": [],
   "source": [
    "# Tokenize a sample input for chat-like generation\n",
    "summary = \"The transportation industry is experiencing vast digitalization as a plethora of technologies are being implemented to improve efficiency, functionality, and safety. Although technological advancements bring many benefits to transportation, integrating cyberspace across transportation sectors has introduced new and deliberate cyber threats. In the past, public agencies assumed digital infrastructure was secured since its vulnerabilities were unknown to adversaries. However, with the expansion of cyberspace, this assumption has become invalid. With the rapid advancement of wireless technologies, transportation systems are increasingly interconnected with both transportation and non-transportation networks in an internet-of-things ecosystem, expanding cyberspace in transportation and increasing threats and vulnerabilities. This study investigates some prominent reasons for the increase in cyber vulnerabilities in transportation. In addition, this study presents various collaborative strategies among stakeholders that could help improve cybersecurity in the transportation industry. These strategies address programmatic and policy aspects and suggest avenues for technological research and development. The latter highlights opportunities for future research to enhance the cybersecurity of transportation systems and infrastructure by leveraging hybrid approaches and emerging technologies.\"\n",
    "\n",
    "# Compose the chat-like prompt\n",
    "prompt = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": summary},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Apply chat template if supported\n",
    "messages = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt = True,  # If you want the template to include generation guidance\n",
    "    tokenize = False  # Return as plain text, not tokenized IDs yet\n",
    ")\n",
    "\n",
    "# Tokenize the formatted prompt\n",
    "inputs = tokenizer(\n",
    "    messages,\n",
    "    truncation = True,\n",
    "    padding = \"longest\",\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdWiC7QUDDK1"
   },
   "outputs": [],
   "source": [
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Generate the responses\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 16,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJMObYBnDJwX"
   },
   "outputs": [],
   "source": [
    "# Decode the model output\n",
    "gen_ids = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "# Extract the generated category from the response\n",
    "category = generated_text.split(\"Category:\", 1)[-1].strip() if \"Category:\" in generated_text else generated_text.strip()\n",
    "\n",
    "# Print the response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHq4qylDQ36x8M54uxshR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
