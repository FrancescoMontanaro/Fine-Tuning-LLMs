{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbMjkhsSyq-"
   },
   "source": [
    "### Installing and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV9rVdkGEFFI"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Any\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, TextStreamer\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.hf import hf_login\n",
    "from src.utils import get_device, set_seed\n",
    "from src.data_processing import build_chat, generate_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz1dCNVaXdjy"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "hf_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5A58TsXOdcF"
   },
   "outputs": [],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHtrAS4G_3c"
   },
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OMAcVUtEWyv"
   },
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "max_length = 448 # Maximum length of the sequences\n",
    "model_id = \"Qwen/Qwen3-1.7B\" # The model ID\n",
    "save_trained_model = False # Whether to save the model after training\n",
    "dataset_path = Path().resolve().parent.parent / \"datasets\" / \"arxiv_dataset.csv\" # Path to the dataset\n",
    "adapter_path = Path().resolve().parent.parent / \"saved_models\" / \"papers_category_classifier_adapter\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJtYT9AAJA9Z"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGd3OTw_JChU"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(\n",
    "    dataset_path,\n",
    "    delimiter = \"|\",\n",
    "    quoting = 3,  # Handle quotes around text\n",
    "    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dataset = dataset[[\n",
    "    \"summary\", # Feature\n",
    "    \"category_description\" # Label\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obTlSyEdJSyN"
   },
   "outputs": [],
   "source": [
    "# Show a subset of the samples\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YwNOg1CHqJp"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDgHdDxaEa5k"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWe7YFh9KM47"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYydkXj3KxYe"
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = hf_dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zTYrCiEuxKo"
   },
   "outputs": [],
   "source": [
    "def preprocess(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Preprocess the examples to build input ids, attention masks, and labels\n",
    "    inputs, masks, labels = [], [], []\n",
    "    \n",
    "    # Iterate through each example and build the chat inputs\n",
    "    for summary, category in zip(examples[\"summary\"], examples[\"category_description\"]):\n",
    "        # Build chat inputs\n",
    "        ids, attn, labs = build_chat(\n",
    "            tokenizer, \n",
    "            max_length,\n",
    "            user_text = summary,\n",
    "            answer_text = f\"<category>{category}</category>\",\n",
    "        )\n",
    "\n",
    "        # Append to the respective lists\n",
    "        inputs.append(ids)\n",
    "        masks.append(attn)\n",
    "        labels.append(labs)\n",
    "        \n",
    "\t# Return the processed inputs as a dictionary\n",
    "    return {\"input_ids\": inputs, \"attention_mask\": masks, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RevlAGKTwuWC"
   },
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"summary\",\"category_description\"])\n",
    "tokenized_test_dataset  = test_dataset.map(preprocess,  batched=True, remove_columns=[\"summary\",\"category_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwZ4CvuLyz4p"
   },
   "outputs": [],
   "source": [
    "# Select a random training sample\n",
    "random_sample = random.choice(tokenized_train_dataset)\n",
    "\n",
    "# Print a random sequence\n",
    "print(\"FULL SEQUENCE:\")\n",
    "print(\"-\" * 20)\n",
    "print(tokenizer.decode(random_sample[\"input_ids\"]))\n",
    "\n",
    "# Print the labels of the random sample\n",
    "print(\"\\nLABEL:\")\n",
    "print(\"-\" * 20)\n",
    "print(tokenizer.decode([l for l in random_sample[\"labels\"] if l != -100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jVoBtk0HwS9"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCk7e6F5w3Id"
   },
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGRRZ6OvE8Ro"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ua74D42HIFj"
   },
   "outputs": [],
   "source": [
    "# LoRA (Low-rank adaptation configurations)\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,                        # Rank of the LoRA matrices\n",
    "    lora_alpha = 32,               # Alpha parameter for scaling\n",
    "    use_rslora = True,             # Use RSLora\n",
    "    lora_dropout = 0.1,            # Dropout probability\n",
    "    target_modules = [             # Target modules to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPgCfrhdH34l"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA (Low-rank adaptation) to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWh5J_2lPX_z"
   },
   "outputs": [],
   "source": [
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxcI8ceiTmzx"
   },
   "outputs": [],
   "source": [
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddnJy6u3IILa"
   },
   "source": [
    "### Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6IqtdVEIJ46"
   },
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./checkpoints/papers_category_classifier\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 3e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 10,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 10,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to = \"none\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "    bf16 = bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX8Id63uNJ0D"
   },
   "outputs": [],
   "source": [
    "# Instantiate the trainer to train the model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_test_dataset\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Pretty print the training results\n",
    "print(trainer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKZO4iY6j5V"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG859UBQ6n_L"
   },
   "outputs": [],
   "source": [
    "if save_trained_model:\n",
    "    # Saving the adapter to the destination path\n",
    "    model.save_pretrained(str(adapter_path))\n",
    "    \n",
    "    # Load the base model first\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map = \"auto\",\n",
    "        low_cpu_mem_usage = True,\n",
    "        quantization_config = quantization_config\n",
    "    )\n",
    "\n",
    "    # Load the LoRA adapter and attach it to the base model\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI9Bm9SoDEOS"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_DtdlICmI9"
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsqhsFpN9qA2"
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize a sample input for chat-like generation\n",
    "summary = (\n",
    "    \"The transportation industry is experiencing vast digitalization as a plethora of technologies are being implemented to improve efficiency, functionality, and safety. \"\n",
    "    \"Although technological advancements bring many benefits to transportation, integrating cyberspace across transportation sectors has introduced new and deliberate cyber threats. \"\n",
    "    \"In the past, public agencies assumed digital infrastructure was secured since its vulnerabilities were unknown to adversaries. \"\n",
    "    \"However, with the expansion of cyberspace, this assumption has become invalid. With the rapid advancement of wireless technologies, transportation systems are increasingly interconnected with both transportation and non-transportation networks in an internet-of-things ecosystem, expanding cyberspace in transportation and increasing threats and vulnerabilities.\" \n",
    "    \"This study investigates some prominent reasons for the increase in cyber vulnerabilities in transportation. In addition, this study presents various collaborative strategies among stakeholders that could help improve cybersecurity in the transportation industry. \"\n",
    "    \"These strategies address programmatic and policy aspects and suggest avenues for technological research and development. \"\n",
    "    \"The latter highlights opportunities for future research to enhance the cybersecurity of transportation systems and infrastructure by leveraging hybrid approaches and emerging technologies.\"\n",
    ")\n",
    "\n",
    "# Generate a response with streaming\n",
    "response = generate_response(\n",
    "    model = model, \n",
    "    tokenizer = tokenizer,\n",
    "    user_message = summary,\n",
    "    max_new_tokens = 16,\n",
    "    stream = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJMObYBnDJwX"
   },
   "outputs": [],
   "source": [
    "# Extract the generated category from the response\n",
    "match = re.search(r\"<category>(.*?)</category>\", response)\n",
    "category = match.group(1).strip() if match else response.strip()\n",
    "\n",
    "# Print the response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHq4qylDQ36x8M54uxshR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
