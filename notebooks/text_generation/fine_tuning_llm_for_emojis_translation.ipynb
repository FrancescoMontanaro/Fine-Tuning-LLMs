{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbMjkhsSyq-"
   },
   "source": [
    "### Installing and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV9rVdkGEFFI"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Any, cast\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import PreTrainedModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.hf import hf_login\n",
    "from src.utils import get_device, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz1dCNVaXdjy"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "hf_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5A58TsXOdcF"
   },
   "outputs": [],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHtrAS4G_3c"
   },
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OMAcVUtEWyv"
   },
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "max_length = 64 # Maximum length of the sequences\n",
    "model_id = \"Qwen/Qwen3-0.6B\" # The model ID\n",
    "dataset_path = Path().resolve().parent.parent / \"datasets\" / \"emoji_translation_dataset.csv\" # Path to the dataset\n",
    "adapter_path = Path().resolve().parent.parent / \"saved_models\" / \"emoji_translation_adapter\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJtYT9AAJA9Z"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGd3OTw_JChU"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(\n",
    "    dataset_path,\n",
    "    delimiter = \",\",  # Use ',' as the delimiter\n",
    "    quoting = 3,  # Handle quotes around text\n",
    "    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",
    ")\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dataset = dataset[[\n",
    "    \"text\", # Feature\n",
    "    \"emoji\" # Label\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obTlSyEdJSyN"
   },
   "outputs": [],
   "source": [
    "# Show all the text in the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Show a subset of the samples\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YwNOg1CHqJp"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDgHdDxaEa5k"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWe7YFh9KM47"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zTYrCiEuxKo"
   },
   "outputs": [],
   "source": [
    "def build_chat(user_text: str, answer_text: str) -> tuple[list[int], list[int], list[int]]:\n",
    "    # Build the chat conversation\n",
    "    system_prompt = {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text into emojis.\"}\n",
    "    user_prompt = {\"role\": \"user\", \"content\": f\"Translate the following text into emojis: {user_text}\"}\n",
    "    assistant_response = {\"role\": \"assistant\", \"content\": answer_text}\n",
    "    \n",
    "    # Create the full conversation\n",
    "    conversation = [\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        assistant_response\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    full_text = tokenizer.apply_chat_template(conversation, add_generation_prompt=False, tokenize=False, enable_thinking=False)\n",
    "\n",
    "    # Build prompt only (without the answer) and apply the chat template\n",
    "    prompt_only = [system_prompt, user_prompt]\n",
    "    prompt_text = tokenizer.apply_chat_template(prompt_only, add_generation_prompt=True, tokenize=False, enable_thinking=False)\n",
    "\n",
    "\t# Tokenize both full and prompt texts\n",
    "    full = tokenizer(full_text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\t# Extract input ids and attention masks\n",
    "    input_ids = full[\"input_ids\"]\n",
    "    attn = full[\"attention_mask\"]\n",
    "\n",
    "\t# Create labels, initialized to -100 (ignore index)\n",
    "    labels = [-100] * len(input_ids)\n",
    "    \n",
    "    # Determine the starting index of the assistant's response\n",
    "    start = len(tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"])\n",
    "    \n",
    "    # Fill labels with input ids for the assistant portion only, ignore padding\n",
    "    for i in range(start, len(input_ids)):\n",
    "        if attn[i] == 1:\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "\t# Return the input ids, attention mask, and labels\n",
    "    return input_ids, attn, labels\n",
    "\n",
    "def preprocess(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Preprocess the examples to build input ids, attention masks, and labels\n",
    "    inputs, masks, labels = [], [], []\n",
    "    \n",
    "    # Iterate through each example and build the chat inputs\n",
    "    for u, y in zip(examples[\"text\"], examples[\"emoji\"]):\n",
    "        # Build chat inputs\n",
    "        ids, attn, labs = build_chat(u, y)\n",
    "        \n",
    "        # Append to the respective lists\n",
    "        inputs.append(ids)\n",
    "        masks.append(attn)\n",
    "        labels.append(labs)\n",
    "        \n",
    "\t# Return the processed inputs as a dictionary\n",
    "    return {\"input_ids\": inputs, \"attention_mask\": masks, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RevlAGKTwuWC"
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Preprocess the dataset to build input ids, attention masks, and labels\n",
    "dataset = hf_dataset.map(preprocess, batched=True, remove_columns=hf_dataset.column_names)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwZ4CvuLyz4p"
   },
   "outputs": [],
   "source": [
    "# Select a random training sample\n",
    "random_sample = random.choice(train_dataset)\n",
    "\n",
    "# Print a random sequence\n",
    "print(\"FULL SEQUENCE:\")\n",
    "print(\"-\" * 20)\n",
    "print(tokenizer.decode(random_sample[\"input_ids\"]))\n",
    "\n",
    "# Print the labels of the random sample\n",
    "print(\"\\nLABEL:\")\n",
    "print(\"-\" * 20)\n",
    "print(tokenizer.decode([l for l in random_sample[\"labels\"] if l != -100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jVoBtk0HwS9"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCk7e6F5w3Id"
   },
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGRRZ6OvE8Ro"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ua74D42HIFj"
   },
   "outputs": [],
   "source": [
    "# LoRA (Low-rank adaptation configurations)\n",
    "lora_config = LoraConfig(\n",
    "    r = 16,                        # Rank of the LoRA matrices\n",
    "    lora_alpha = 32,               # Alpha parameter for scaling\n",
    "    use_rslora = True,             # Use RSLora\n",
    "    lora_dropout = 0.1,            # Dropout probability\n",
    "    target_modules = [             # Target modules to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPgCfrhdH34l"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA (Low-rank adaptation) to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWh5J_2lPX_z"
   },
   "outputs": [],
   "source": [
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxcI8ceiTmzx"
   },
   "outputs": [],
   "source": [
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddnJy6u3IILa"
   },
   "source": [
    "### Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6IqtdVEIJ46"
   },
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# SFTTrainer config \n",
    "sft_config = SFTConfig(\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 50,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    logging_steps = 10,\n",
    "    eval_strategy = \"steps\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "\tbf16 = bf16,\n",
    "\tweight_decay = 0.01,\n",
    " \tlr_scheduler_type = \"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX8Id63uNJ0D"
   },
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = cast(PreTrainedModel, model),\n",
    "    args = sft_config,\n",
    "    train_dataset = train_dataset, \n",
    "    eval_dataset = test_dataset\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "# Pretty print the training results\n",
    "print(trainer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKZO4iY6j5V"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG859UBQ6n_L"
   },
   "outputs": [],
   "source": [
    "# Saving the adapter to the destination path\n",
    "model.save_pretrained(str(adapter_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKGMC3q08t6o"
   },
   "source": [
    "### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_DtdlICmI9"
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGEk88YD838p"
   },
   "outputs": [],
   "source": [
    "# Load the base model first\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter and attach it to the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsqhsFpN9qA2"
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI9Bm9SoDEOS"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnv0OIRZEpdO"
   },
   "outputs": [],
   "source": [
    "# Compose the chat-like prompt\n",
    "prompt = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text into emojis.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Translate the following text into emojis: I love programming and coffee.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Apply chat template if supported\n",
    "messages = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False\n",
    ")\n",
    "\n",
    "# Tokenize the formatted prompt\n",
    "inputs = tokenizer(\n",
    "    messages,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdWiC7QUDDK1"
   },
   "outputs": [],
   "source": [
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Generate the responses\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 16,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJMObYBnDJwX"
   },
   "outputs": [],
   "source": [
    "# Decode the model output\n",
    "gen_ids = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "# Extract the generated category from the response\n",
    "match = re.search(r\"<category>(.*?)</category>\", generated_text)\n",
    "category = match.group(1).strip() if match else generated_text.strip()\n",
    "\n",
    "# Print the response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNuHq4qylDQ36x8M54uxshR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
