{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f123fe2",
   "metadata": {},
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "# Import local dependencies\n",
    "from src.utils import get_device, set_seed\n",
    "from src.data_processing import generate_response\n",
    "from src.hf import hf_login, load_hf_dataset, dataset_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae9fc2",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "hf_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b82ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "use_cuda = torch.cuda.is_available() and \"cuda\" in str(device).lower()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aded5",
   "metadata": {},
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d379eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" # The model ID\n",
    "dataset_name = \"openai/gsm8k\" # The dataset name on Hugging Face Hub\n",
    "model_path = Path().resolve().parent.parent / \"saved_models\" / f\"{model_id.split('/')[-1]}_grpo\" # Path to save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e025e",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face Hub\n",
    "train_dataset = load_hf_dataset(dataset_name, config_name=\"main\", split=\"train\")\n",
    "test_dataset = load_hf_dataset(dataset_name, config_name=\"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bad1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame for easier manipulation\n",
    "train_dataset_df = dataset_to_pandas(train_dataset)\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 0)         \n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(train_dataset_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6479902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(completions: list[dict], ground_truth: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Custom reward function that checks if the model's output matches the ground truth answer.\n",
    "    \n",
    "    Args:\n",
    "        completions (list[dict]): List of model completions, each a dict with 'content' key.\n",
    "        ground_truth (list[str]): List of ground truth answers.\n",
    "        \n",
    "    Returns:\n",
    "        list[float]: List of rewards (1.0 for correct, 0.0 for incorrect).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regular expression to capture content inside \\boxed{}\n",
    "    matches = [re.search(r\"\\\\boxed\\{(.*?)\\}\", completion[0]['content']) for completion in completions]\n",
    "    contents = [match.group(1) if match else \"\" for match in matches]\n",
    "    \n",
    "    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "    return [1.0 if c == gt else 0.0 for c, gt in zip(contents, ground_truth)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Post-processes a dataset example to extract the ground truth answer and format the prompt.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): A dictionary containing the dataset example with 'question' and 'answer' keys.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The modified example with 'ground_truth' and 'prompt' keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the system prompt\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful assistant that solves problems step-by-step. \"\n",
    "        \"Always include the final numeric answer inside \\\\boxed{}.\"\n",
    "    )\n",
    "    \n",
    "    # Extract the ground truth answer using regex\n",
    "    match = re.search(r\"####\\s*(-?\\d+)\", example[\"answer\"])\n",
    "    example[\"ground_truth\"] = match.group(1) if match else None\n",
    "    \n",
    "    # Format the prompt with system and user roles\n",
    "    example[\"prompt\"] = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "    ]\n",
    "\n",
    "    # Return the modified example\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea197f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post-processing to the datasets\n",
    "train_dataset = train_dataset.map(post_processing).remove_columns([\"question\", \"answer\"])\n",
    "test_dataset = test_dataset.map(post_processing).remove_columns([\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b21235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the processed training dataset to a pandas DataFrame\n",
    "train_df = dataset_to_pandas(train_dataset)\n",
    "\n",
    "# Display the first few rows of the processed training DataFrame\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130d439",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b550399",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111aa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e00c5",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision settings\n",
    "use_pin_memory = bool(use_cuda)\n",
    "bf16 = bool(use_cuda and torch.cuda.is_bf16_supported())\n",
    "\n",
    "# Define GRPO training configuration\n",
    "config = GRPOConfig(\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    eval_strategy = \"steps\",\n",
    "    num_generations = 4,\n",
    "    num_train_epochs = 1,\n",
    "    learning_rate = 5e-6,\n",
    "    logging_steps = 2,\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    dataloader_pin_memory = use_pin_memory,\n",
    "    bf16 = bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GRPOTrainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    args = config,\n",
    "    model = model,\n",
    "    reward_funcs = reward_func,\n",
    "    processing_class = tokenizer,  \n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf52717",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset for evaluation\n",
    "for example in test_dataset:\n",
    "    # Ensure the example is a dictionary\n",
    "    assert isinstance(example, dict)\n",
    "    \n",
    "    # Get the input prompt and ground truth answer\n",
    "    input_prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "\n",
    "    # Run the model to generate an answer\n",
    "    response = generate_response(\n",
    "        model = model, \n",
    "        tokenizer = tokenizer,\n",
    "        full_message = input_prompt,\n",
    "        stream = True\n",
    "    ) \n",
    "    \n",
    "    # Store the predictions and ground truths\n",
    "    all_preds.append([{\"role\": \"assistant\", \"content\": response}])\n",
    "    all_labels.append(ground_truth)\n",
    "    \n",
    "    # Print the ground truth\n",
    "    print(\"Ground truth: \", ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate using reward_func\n",
    "rewards = reward_func(all_preds, all_labels)\n",
    "\n",
    "# Compute and display accuracy\n",
    "accuracy = sum(rewards) / len(rewards)\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
