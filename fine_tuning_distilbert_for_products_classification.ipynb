{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbMjkhsSyq-"
   },
   "source": [
    "### Installing and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QV9rVdkGEFFI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from typing import Dict, Any\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "\n",
    "# Import local dependencies\n",
    "from utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlHtrAS4G_3c"
   },
   "source": [
    "### Constants, hyperparameters and model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_OMAcVUtEWyv"
   },
   "outputs": [],
   "source": [
    "seed = 42 # Seed for reproducibility\n",
    "test_size = 0.2 # Train-test split percentage\n",
    "model_id = \"bert-base-uncased\" # The model ID of the Llama model\n",
    "dataset_path = \"./datasets/iphone_products.csv\" # The path to the dataset\n",
    "model_path = \"./saved_models/iphone_products_classifier\" # Path to save the trained model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E5A58TsXOdcF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "# Get the device available on the system\n",
    "device = get_device()\n",
    "\n",
    "# Print the detected device\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJtYT9AAJA9Z"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UGd3OTw_JChU"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(\n",
    "    dataset_path,\n",
    "    delimiter = \",\",\n",
    "    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0FREMlY2hd7m"
   },
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "obTlSyEdJSyN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iphone</td>\n",
       "      <td>Cover magsafe in pelle iPhone 12 mini</td>\n",
       "      <td>derived_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iphone</td>\n",
       "      <td>Cover GUESS Hard x Iphone 12 e 12 PRO</td>\n",
       "      <td>derived_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iphone</td>\n",
       "      <td>Display iphone 12 pro max</td>\n",
       "      <td>derived_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iphone</td>\n",
       "      <td>display x IPHONE 12 pro max nuovo</td>\n",
       "      <td>derived_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iphone</td>\n",
       "      <td>Iphone 12 pro 256 danneggiato</td>\n",
       "      <td>target_product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product                                  title            label\n",
       "0  iphone  Cover magsafe in pelle iPhone 12 mini  derived_product\n",
       "1  iphone  Cover GUESS Hard x Iphone 12 e 12 PRO  derived_product\n",
       "2  iphone              Display iphone 12 pro max  derived_product\n",
       "3  iphone      display x IPHONE 12 pro max nuovo  derived_product\n",
       "4  iphone          Iphone 12 pro 256 danneggiato   target_product"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a subset of the samples\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YwNOg1CHqJp"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FDgHdDxaEa5k"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1024bf8c5a5948dba274a4a17133a62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c871ad6b4948928dfaebdf318bc54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b3a06cdd9549868680168a0d42bc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb62cc901b0846e49825b06f90852ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, paddind_side=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWe7YFh9KM47"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jEzN8p-vhFmp"
   },
   "outputs": [],
   "source": [
    "# Concatenate the product and its title\n",
    "dataset[\"summary\"] = dataset[\"product\"] + \" - \" + dataset[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bU876oPwJ0WS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Instantiate  the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target column (category_description) into numeric labels\n",
    "dataset.loc[:, \"label\"] = label_encoder.fit_transform(dataset[\"label\"])\n",
    "\n",
    "# Extract and print the total number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Total number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FYydkXj3KxYe"
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = hf_dataset.train_test_split(test_size=test_size, seed=seed).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Eh7bXTEOMEbn"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df49db8843fc45f39370c8d353a2a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdc5c9f55304d978aac07929d8ccd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 48\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess(examples: Dict[str, Any], max_length: int = 48) -> Dict[str, Any]:\n",
    "    # Tokenize the input sequences\n",
    "    return tokenizer(\n",
    "        examples[\"summary\"],\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = max_length\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"product\", \"title\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"product\", \"title\"])\n",
    "\n",
    "# Display the sequence length\n",
    "print(f\"Sequence length: {len(tokenized_train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jVoBtk0HwS9"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BGRRZ6OvE8Ro"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a36a3ca27a45e0b7cf05549b49af83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a893809a3d314be684c7c05b3050c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GOimK-VDZJzd"
   },
   "outputs": [],
   "source": [
    "# Move the model to the taret device\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yBQ4ApsxlRL_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddnJy6u3IILa"
   },
   "source": [
    "### Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RJa1Dkv4NRT6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742589ecb57740348e6d261a23f59c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "# Define a custum function to compute the metrics\n",
    "def compute_metrics(eval_pred: torch.Tensor) -> torch.Tensor:\n",
    "    # Extract the logits and the lables from the output of the model\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Extract the predictions for each sample\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Compute and return the accuarcy\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6IqtdVEIJ46"
   },
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./iphone_products_classifier\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 3e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 20,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 10,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"accuracy\",\n",
    "    greater_is_better = True,\n",
    "    report_to = \"none\",\n",
    "    pin_memory = False,\n",
    "    fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PX8Id63uNJ0D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='412' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 412/1180 01:44 < 03:15, 3.93 it/s, Epoch 6.97/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/francescomontanaro/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     model = model,\n\u001b[32m      4\u001b[39m     args = training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     compute_metrics = compute_metrics\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Trainin the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/transformers/trainer.py:2752\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2749\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n\u001b[32m   2750\u001b[39m         \u001b[38;5;28mself\u001b[39m.lr_scheduler.step()\n\u001b[32m-> \u001b[39m\u001b[32m2752\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2753\u001b[39m \u001b[38;5;28mself\u001b[39m.state.global_step += \u001b[32m1\u001b[39m\n\u001b[32m   2754\u001b[39m \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m) / steps_in_epoch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2952\u001b[39m, in \u001b[36mModule.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_replica\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2945\u001b[39m     warnings.warn(\n\u001b[32m   2946\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCalling .zero_grad() from a module created with nn.DataParallel() has no effect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2947\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe parameters are copied (in a differentiable manner) from the original module. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis means they are not leaf nodes in autograd and so don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt accumulate gradients. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2949\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you need gradients in your forward method, consider using autograd.grad instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2950\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2952\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   2954\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mset_to_none\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2678\u001b[39m, in \u001b[36mModule.parameters\u001b[39m\u001b[34m(self, recurse)\u001b[39m\n\u001b[32m   2656\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Parameter]:\n\u001b[32m   2657\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters.\u001b[39;00m\n\u001b[32m   2658\u001b[39m \n\u001b[32m   2659\u001b[39m \u001b[33;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2676\u001b[39m \n\u001b[32m   2677\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2711\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2684\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2685\u001b[39m \n\u001b[32m   2686\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2703\u001b[39m \n\u001b[32m   2704\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2705\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2706\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2707\u001b[39m     prefix=prefix,\n\u001b[32m   2708\u001b[39m     recurse=recurse,\n\u001b[32m   2709\u001b[39m     remove_duplicate=remove_duplicate,\n\u001b[32m   2710\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2711\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2646\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2640\u001b[39m memo = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m   2641\u001b[39m modules = (\n\u001b[32m   2642\u001b[39m     \u001b[38;5;28mself\u001b[39m.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n\u001b[32m   2643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recurse\n\u001b[32m   2644\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[32m   2645\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_members_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2868\u001b[39m, in \u001b[36mModule.named_modules\u001b[39m\u001b[34m(self, memo, prefix, remove_duplicate)\u001b[39m\n\u001b[32m   2866\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2867\u001b[39m submodule_prefix = prefix + (\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name\n\u001b[32m-> \u001b[39m\u001b[32m2868\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m module.named_modules(\n\u001b[32m   2869\u001b[39m     memo, submodule_prefix, remove_duplicate\n\u001b[32m   2870\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2868\u001b[39m, in \u001b[36mModule.named_modules\u001b[39m\u001b[34m(self, memo, prefix, remove_duplicate)\u001b[39m\n\u001b[32m   2866\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2867\u001b[39m submodule_prefix = prefix + (\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name\n\u001b[32m-> \u001b[39m\u001b[32m2868\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m module.named_modules(\n\u001b[32m   2869\u001b[39m     memo, submodule_prefix, remove_duplicate\n\u001b[32m   2870\u001b[39m )\n",
      "    \u001b[31m[... skipping similar frames: Module.named_modules at line 2868 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2868\u001b[39m, in \u001b[36mModule.named_modules\u001b[39m\u001b[34m(self, memo, prefix, remove_duplicate)\u001b[39m\n\u001b[32m   2866\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2867\u001b[39m submodule_prefix = prefix + (\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name\n\u001b[32m-> \u001b[39m\u001b[32m2868\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m module.named_modules(\n\u001b[32m   2869\u001b[39m     memo, submodule_prefix, remove_duplicate\n\u001b[32m   2870\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Fine-Tuning-LLMs/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2823\u001b[39m, in \u001b[36mModule.named_modules\u001b[39m\u001b[34m(self, memo, prefix, remove_duplicate)\u001b[39m\n\u001b[32m   2820\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_modules():\n\u001b[32m   2821\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m module\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnamed_modules\u001b[39m(\n\u001b[32m   2824\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2825\u001b[39m     memo: Optional[\u001b[38;5;28mset\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2826\u001b[39m     prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2827\u001b[39m     remove_duplicate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2828\u001b[39m ):\n\u001b[32m   2829\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[32m   2830\u001b[39m \n\u001b[32m   2831\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2858\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the trainer to train the model\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_test_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "# Trainin the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKZO4iY6j5V"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG859UBQ6n_L"
   },
   "outputs": [],
   "source": [
    "# Saving the adapter to the destination path\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKGMC3q08t6o"
   },
   "source": [
    "### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE2oJ42U9BYa"
   },
   "outputs": [],
   "source": [
    "# Define the quantization configurations of the model (only for CUDA devices)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGEk88YD838p"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage = True,\n",
    "    quantization_config = quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UYNc3xkkjTc"
   },
   "outputs": [],
   "source": [
    "# Move the fine-tuned model to the target device\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI9Bm9SoDEOS"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etrKOfCGD0GR"
   },
   "outputs": [],
   "source": [
    "# Tokenize a sample input\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        \"Iphone 12 pro 256 danneggiato\", # 1\n",
    "        \"Cover antigraffio per iPhone\", # 0\n",
    "        \"Drone per iphone con custodia\", # 0\n",
    "        \"iPhone 13 mini 500TB Rosa\", # 1\n",
    "        \"Set di pellicole per iphone 15 pro max\" # 0\n",
    "    ],\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors=\"pt\"\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdWiC7QUDDK1"
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # Compute the output of the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Extract the predictions\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert the predictions to a numpy array\n",
    "predictions = predictions.cpu().numpy()\n",
    "\n",
    "# Convert the predicted labels to the corresponding categories\n",
    "predicted_categories = label_encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7UQeY7aD22a"
   },
   "outputs": [],
   "source": [
    "# Display the predicted categories\n",
    "for idx, predicted_category in enumerate(predicted_categories):\n",
    "    print(f\"Sample {idx + 1} --> Predicted label: {predictions[idx]} | Predicted Category: {predicted_category}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMni95n5T23l41KCyRLAnHZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
