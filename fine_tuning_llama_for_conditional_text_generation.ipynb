{"cells":[{"cell_type":"markdown","metadata":{"id":"ZAbMjkhsSyq-"},"source":["### Installing and importing the required modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sh1Mxz3wD9cI"},"outputs":[],"source":["%%capture\n","!pip install evaluate bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QV9rVdkGEFFI"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from evaluate import load\n","from typing import Dict, Any\n","from datasets import Dataset\n","from google.colab import drive\n","from huggingface_hub import login\n","from google.colab import userdata\n","from peft import LoraConfig, PeftModel, get_peft_model\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer"]},{"cell_type":"markdown","metadata":{"id":"ewOtjy0DNz52"},"source":["### Setting up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9E_WHBeNwl9"},"outputs":[],"source":["# Mounting the drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pz1dCNVaXdjy"},"outputs":[],"source":["# Extract the hugging face token from the user data\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","# Check if the HF token has been provided\n","if not HF_TOKEN:\n","  # Raise an exception if the HF token was not provided\n","  raise Exception(\"Token is not set. Please save the token first.\")\n","\n","# Authenticate with hugging face\n","login(HF_TOKEN)\n","\n","# Login successful\n","print(\"Successfully logged in to Hugging Face!\")"]},{"cell_type":"markdown","metadata":{"id":"BlHtrAS4G_3c"},"source":["### Constants, hyperparameters and model configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OMAcVUtEWyv"},"outputs":[],"source":["seed = 42 # Seed for reproducibility\n","test_size = 0.2 # Train-test split percentage\n","max_length = 512 # Maximum length of the sequences\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # The device to run the model on\n","model_id = \"meta-llama/Llama-3.2-3B-Instruct\" # The model ID of the Llama model\n","dataset_path = \"/content/drive/MyDrive/Colab Notebooks/FineTuningLLM/datasets/arxiv_dataset.csv\" # The path to the dataset\n","adapter_path = \"/content/drive/MyDrive/Colab Notebooks/FineTuningLLM/saved_models/papers_category_classifier_adapter\" # Path to save the trained model to"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5A58TsXOdcF"},"outputs":[],"source":["# Print the detected device\n","print(f\"Detected device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"iJtYT9AAJA9Z"},"source":["### Data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGd3OTw_JChU"},"outputs":[],"source":["# Load the dataset into a pandas DataFrame\n","dataset = pd.read_csv(\n","    dataset_path,\n","    delimiter = \"|\",\n","    quoting = 3,  # Handle quotes around text\n","    on_bad_lines = \"skip\"  # Skip problematic lines if necessary\n",")\n","\n","# Keep only the relevant columns\n","dataset = dataset[[\n","    \"summary\", # Feature\n","    \"category_description\" # Label\n","]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obTlSyEdJSyN"},"outputs":[],"source":["# Show a subset of the samples\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"6YwNOg1CHqJp"},"source":["### Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDgHdDxaEa5k"},"outputs":[],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Set the padding token to the end of the sequence\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{"id":"VWe7YFh9KM47"},"source":["### Preprocess data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYydkXj3KxYe"},"outputs":[],"source":["# Convert the Pandas DataFrame to a Hugging Face Dataset\n","hf_dataset = Dataset.from_pandas(dataset)\n","\n","# Train-test split\n","train_dataset, test_dataset = hf_dataset.train_test_split(test_size=test_size, seed=seed).values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zTYrCiEuxKo"},"outputs":[],"source":["def preprocess(examples) -> dict:\n","    # Compose the prompts\n","    prompts = [\n","        [\n","            {\"role\": \"user\", \"content\": f\"Given the following summary, predict the category: {summary}\"},\n","            {\"role\": \"assistant\", \"content\": \"\"}\n","        ]\n","        for summary in examples[\"summary\"]\n","    ]\n","\n","    # Extract the target responses\n","    target_responses = examples[\"category_description\"]\n","\n","    # Apply the chat template to the prompts\n","    chat_templates = tokenizer.apply_chat_template(\n","        prompts,\n","        add_generation_prompt = True,\n","        tokenize = False\n","    )\n","\n","    # Tokenize the full response of the assistant\n","    input_ids_tokenized = tokenizer(\n","        chat_templates,\n","        truncation = True,\n","        padding = \"max_length\",\n","        max_length = max_length,\n","        padding_side = \"right\",\n","        return_tensors = \"pt\"\n","    )['input_ids']\n","\n","    # Tokenize only the response\n","    labels_tokenized = tokenizer(\n","        [f\"Category: {response}{tokenizer.eos_token}\" for response in target_responses],\n","        truncation = True,\n","        padding = \"max_length\",\n","        max_length = max_length,\n","        padding_side = \"right\",\n","        return_tensors = \"pt\"\n","    )['input_ids']\n","\n","    # Mask all padding tokens except the first\n","    for i, label_row in enumerate(labels_tokenized):\n","        padding_mask = label_row == tokenizer.pad_token_id  # Identify padding tokens\n","        padding_indices = padding_mask.nonzero(as_tuple=True)[0]  # Indices of padding tokens\n","        if len(padding_indices) > 1:  # If there are multiple padding tokens\n","            labels_tokenized[i, padding_indices[1:]] = -100  # Mask all except the first padding token\n","\n","    # Shift the input and target tokens\n","    input_ids_tokenized = input_ids_tokenized[:, :-1] # (start) to (end - 1)\n","    labels_tokenized = labels_tokenized[:, 1:] # (start + 1) to end\n","\n","    # Create the attention mask\n","    attention_mask = input_ids_tokenized.ne(tokenizer.pad_token_id)\n","\n","    # Return the output data\n","    return {\n","        \"input_ids\": input_ids_tokenized,\n","        \"labels\": labels_tokenized,\n","        \"attention_mask\": attention_mask\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RevlAGKTwuWC"},"outputs":[],"source":["# Preprocess the dataset\n","tokenized_train_dataset = train_dataset.map(preprocess, batched=True)\n","tokenized_test_dataset = test_dataset.map(preprocess, batched=True)\n","\n","# Remove unnecessary columns\n","tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"summary\", \"category_description\"])\n","tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"summary\", \"category_description\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwZ4CvuLyz4p"},"outputs":[],"source":["# Select a random training sample\n","random_sample = random.choice(tokenized_train_dataset)\n","\n","# Print a random sequence\n","print(\"INPUT SEQUENCE\")\n","print(\"-\"*15)\n","print(tokenizer.decode(random_sample[\"input_ids\"]))\n","\n","# Print a random sequence\n","print(\"\\nOUTPUT SEQUENCE\")\n","print(\"-\"*15)\n","print(tokenizer.decode([\n","    tokenizer.pad_token_id if token == -100 else token\n","    for token in random_sample[\"labels\"]\n","]))"]},{"cell_type":"markdown","metadata":{"id":"3Yzj6tCX1Wng"},"source":[]},{"cell_type":"markdown","metadata":{"id":"8jVoBtk0HwS9"},"source":["### Building the model"]},{"cell_type":"markdown","metadata":{"id":"QKZhzQNvHssF"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCk7e6F5w3Id"},"outputs":[],"source":["# Define the quantization configurations of the model (only for CUDA devices)\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_quant_type = 'nf4',\n","    bnb_4bit_compute_dtype = torch.float16,\n","    bnb_4bit_use_double_quant = True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGRRZ6OvE8Ro"},"outputs":[],"source":["# Load the model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    low_cpu_mem_usage = True,\n","    quantization_config = quantization_config,\n","    device_map = \"auto\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ua74D42HIFj"},"outputs":[],"source":["# LoRA (Low-rank adaptation configurations)\n","lora_config = LoraConfig(\n","    r = 16,                        # Rank of the LoRA matrices\n","    lora_alpha = 32,               # Alpha parameter for scaling\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"],\n","    use_rslora = True,\n","    lora_dropout = 0.1             # Dropout probability\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPgCfrhdH34l"},"outputs":[],"source":["# Apply LoRA (Low-rank adaptation) to the model\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWh5J_2lPX_z"},"outputs":[],"source":["# Print trainable parameters\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxcI8ceiTmzx"},"outputs":[],"source":["# Print the model\n","model"]},{"cell_type":"markdown","metadata":{"id":"ddnJy6u3IILa"},"source":["### Trainig the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NSMmP-15Alj"},"outputs":[],"source":["# Load the accuracy metric\n","accuracy_metric = load(\"accuracy\")\n","\n","# Define a custum function to compute the metrics\n","def compute_metrics(eval_pred: torch.Tensor) -> torch.Tensor:\n","    # Extract the logits and the lables from the output of the model\n","    logits, labels = eval_pred\n","\n","    # Extract the predictions for each sample\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Compute and return the accuarcy\n","    return accuracy_metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6IqtdVEIJ46"},"outputs":[],"source":["# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir = \"./papers_category_classifier\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    logging_dir = \"./logs\",\n","    logging_strategy = \"epoch\",\n","    learning_rate = 3e-4,\n","    per_device_train_batch_size = 4,\n","    per_device_eval_batch_size = 4,\n","    num_train_epochs = 10,\n","    weight_decay = 0.01,\n","    save_total_limit = 10,\n","    report_to = \"none\",\n","    fp16 = True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX8Id63uNJ0D"},"outputs":[],"source":["# Instantiate the trainer to train the model\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = tokenized_train_dataset,\n","    eval_dataset = tokenized_test_dataset\n",")\n","\n","# Training the model\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"NXKZO4iY6j5V"},"source":["### Save the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IG859UBQ6n_L"},"outputs":[],"source":["# Saving the adapter to the destination path\n","model.save_pretrained(adapter_path)"]},{"cell_type":"markdown","metadata":{"id":"wKGMC3q08t6o"},"source":["### Load the fine-tuned model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0_DtdlICmI9"},"outputs":[],"source":["# Clear GPU cache\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGEk88YD838p"},"outputs":[],"source":["# Load the base model first\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map = \"auto\",\n","    low_cpu_mem_usage = True,\n","    quantization_config = quantization_config\n",")\n","\n","# Load the LoRA adapter and attach it to the base model\n","model = PeftModel.from_pretrained(model, adapter_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsqhsFpN9qA2"},"outputs":[],"source":["# Set the model to evaluation mode\n","model.eval();"]},{"cell_type":"markdown","metadata":{"id":"YI9Bm9SoDEOS"},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etrKOfCGD0GR"},"outputs":[],"source":["# Tokenize a sample input\n","inputs = tokenizer(\n","    \"The transportation industry is experiencing vast digitalization as a plethora of technologies are being implemented to improve efficiency, functionality, and safety. Although technological advancements bring many benefits to transportation, integrating cyberspace across transportation sectors has introduced new and deliberate cyber threats. In the past, public agencies assumed digital infrastructure was secured since its vulnerabilities were unknown to adversaries. However, with the expansion of cyberspace, this assumption has become invalid. With the rapid advancement of wireless technologies, transportation systems are increasingly interconnected with both transportation and non-transportation networks in an internet-of-things ecosystem, expanding cyberspace in transportation and increasing threats and vulnerabilities. This study investigates some prominent reasons for the increase in cyber vulnerabilities in transportation. In addition, this study presents various collaborative strategies among stakeholders that could help improve cybersecurity in the transportation industry. These strategies address programmatic and policy aspects and suggest avenues for technological research and development. The latter highlights opportunities for future research to enhance the cybersecurity of transportation systems and infrastructure by leveraging hybrid approaches and emerging technologies.\",\n","    return_tensors = \"pt\"\n",").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnv0OIRZEpdO"},"outputs":[],"source":["# Tokenize a sample input for chat-like generation\n","summary = \"The transportation industry is experiencing vast digitalization as a plethora of technologies are being implemented to improve efficiency, functionality, and safety. Although technological advancements bring many benefits to transportation, integrating cyberspace across transportation sectors has introduced new and deliberate cyber threats. In the past, public agencies assumed digital infrastructure was secured since its vulnerabilities were unknown to adversaries. However, with the expansion of cyberspace, this assumption has become invalid. With the rapid advancement of wireless technologies, transportation systems are increasingly interconnected with both transportation and non-transportation networks in an internet-of-things ecosystem, expanding cyberspace in transportation and increasing threats and vulnerabilities. This study investigates some prominent reasons for the increase in cyber vulnerabilities in transportation. In addition, this study presents various collaborative strategies among stakeholders that could help improve cybersecurity in the transportation industry. These strategies address programmatic and policy aspects and suggest avenues for technological research and development. The latter highlights opportunities for future research to enhance the cybersecurity of transportation systems and infrastructure by leveraging hybrid approaches and emerging technologies.\"\n","\n","# Compose the chat-like prompt\n","prompts = [\n","    [\n","        {\"role\": \"user\", \"content\": f\"Given the following summary, predict the category: {summary}\"},\n","        {\"role\": \"assistant\", \"content\": \"\"}\n","    ]\n","]\n","\n","# Apply chat template if supported\n","formatted_prompts = tokenizer.apply_chat_template(\n","    prompts,\n","    add_generation_prompt = True,  # If you want the template to include generation guidance\n","    tokenize = False  # Return as plain text, not tokenized IDs yet\n",")\n","\n","# Tokenize the formatted prompt\n","inputs = tokenizer(\n","    formatted_prompts,\n","    truncation = True,\n","    padding = \"max_length\",\n","    max_length = max_length,\n","    padding_side = \"left\",\n","    return_tensors = \"pt\"\n",").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCsw8ghOR2hV"},"outputs":[],"source":["print(tokenizer.decode(inputs[\"input_ids\"][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdWiC7QUDDK1"},"outputs":[],"source":["# Generate the responses\n","outputs = model.generate(\n","    inputs[\"input_ids\"],\n","    attention_mask = inputs[\"attention_mask\"],\n","    max_new_tokens = 100,\n","    eos_token_id = tokenizer.eos_token_id,\n","    pad_token_id = tokenizer.pad_token_id,\n","    temperature = 0.7,\n","    top_k = 50,\n","    top_p = 0.9,\n","    repetition_penalty = 1.2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJMObYBnDJwX"},"outputs":[],"source":["# Decode the model output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Print the response\n","print(generated_text)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNuHq4qylDQ36x8M54uxshR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}